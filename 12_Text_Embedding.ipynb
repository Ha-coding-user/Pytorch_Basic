{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-1 NLP에서의 One-hot Encoding</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[단어 집합]  \n",
    "정의: 서로 다른 단어들의 집합  \n",
    "> book과 books도 다른 단어!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One-hot encoding?\n",
    "One-hot Encoding: 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 index에 1의 값을 부여하고, 다른 index에는 0을 부여하는 단어의 벡터 표현 방식  \n",
    "\n",
    "[과정]  \n",
    "(1) 각 단어에 고유한 index를 부여(정수 인코딩)  \n",
    "(2) 표현하고 싶은 단어의 index의 위치에 1을 부여하고, 다른 단어의 index의 위치에는 0을 부여  \n",
    "\n",
    "[예시]  \n",
    "**문장: 나는 자연어 처리를 배운다**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "# 각 토큰에 대해 고유 index를 부여\n",
    "word2index = {}\n",
    "\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*len(word2index)\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding('자연어', word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-hot Encoding의 한계\n",
    "\n",
    "1. 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어남  \n",
    "즉, 벡터의 차원이 계속 늘어남  \n",
    "2. 단어의 유사도를 표현하지 못함 -> 검색 시스템 등에서 사용할 수 없음..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-2 Word Embedding</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sparse Representation\n",
    "정의: 벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법  \n",
    "문제점: 단어의 개수가 늘어나면 벡터의 차원이 한없이 커짐 -> 공간적 낭비를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 벡터 생성\n",
    "dog = torch.FloatTensor([1, 0, 0, 0, 0])\n",
    "cat = torch.FloatTensor([0, 1, 0, 0, 0])\n",
    "computer = torch.FloatTensor([0, 0, 1, 0, 0])\n",
    "netbook = torch.FloatTensor([0, 0, 0, 1, 0])\n",
    "book = torch.FloatTensor([0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도\n",
    "print(torch.cosine_similarity(dog, cat, dim=0))\n",
    "print(torch.cosine_similarity(cat, computer, dim=0))\n",
    "print(torch.cosine_similarity(computer, netbook, dim=0))\n",
    "print(torch.cosine_similarity(netbook, book, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Cosine-Similarity를 이용해 One-hot Vector의 유사도를 측정할 수 없음</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dense Representation\n",
    "정의: 벡터의 차원을 단어 집합의 크기로 상정하지 않음 -> 사용자가 설정한 값으로 벡터 표현 차원을 맞춤. 또한 0과 1만 가진 값이 아닌 실수값을 가짐  \n",
    "\n",
    "[예시]  \n",
    "강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embedding\n",
    "정의: 단어를 밀집 벡터의 형태로 표현하는 방법  \n",
    "방법론: LSA, Word2Vec, FastText, Glove\n",
    "\n",
    "**[One-hot Vector vs Embedding Vector]**  \n",
    "|-|One-hot Vector|Embedding Vector|\n",
    "|---|---|---|\n",
    "|차원|고차원(단어집합의 크기)|저차원|\n",
    "|다른 표현|희소 벡터의 일종|밀집 벡터의 일종|\n",
    "|표현 방법|수동|훈련 데이터로부터 학습|\n",
    "|값의 타입|1과 0|실수|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-3 Word2Vec</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sparse Representation\n",
    "벡터 또는 행렬의 값이 대부분이 0으로 표현되는 희소 표현 방법은 각 단어간 유사성을 표현할 수 없다는 단점이 존재  \n",
    "이를 위해 대안으로 **분산 표현(distributed representation)** 사용  \n",
    "또한 분산 표현을 이용해 단어의 유사도를 벡터화하는 작업은 word embedding에 속함  \n",
    "그리고 이는 저차원을 가지므로 밀집 벡터(dense vector)에도 속함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distributed Representation\n",
    "정의: 분포 가설이라는 가정 하에 만들어진 표현 방법\n",
    "> **분포 가설: 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가짐**\n",
    "\n",
    "단어의 의미를 여러 차원에다가 분산하여 표현한 방법으로 정의할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CBOW(Continuous Bag of Words)\n",
    "\n",
    "Word2Vec에는 CBOW, Skip-Gram 두 가지 방식이 존재  \n",
    "> CBOW: 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방식  \n",
    "> Skip-Gram: 중간에 있는 단어로 주변 단어들을 예측하는 방법  \n",
    "\n",
    "[CBOW]  \n",
    "예문: \"The fat cat sat on the mat\"  \n",
    "[\"The\", \"fat\", \"cat\", \"on\", \"the\", \"mat\"]으로부터 sat을 예측하는 것은 CBOW가 하는 일  \n",
    "\"sat\" : 중심 단어(center word)\n",
    "예측에 사용되는 단어들 : 주변 단어(context word)\n",
    "\n",
    "<p align=\"center\"><img src=https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG></p>\n",
    "window : 중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지의 범위  \n",
    "이 후 window를 계속 움직여서 주변 단어와 중심 단어 선택을 바꿔가며 학습을 위한 dataset을 만들 수 있는데, 이 방법을 sliding-window라고 함  \n",
    "\n",
    "- CBOW 의 인공신경망\n",
    "  <p align=\"center\"><img src=https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG></p>\n",
    "  입력층의 입력으로 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 One-hot Vector가 들어가고, 출력층에서 예측하고자 하는 중간 단어의 One-hot Vector 필요  \n",
    "  <br>\n",
    "  하지만 이는 DL Model은 아님 -> hidden layer가 1개만 존재(Shallow Neural Network라 부름)  \n",
    "  또한 Word2Vec의 hidden layer는 Activation Funcion이 필요하지 않음  \n",
    "  여기선 hidden layer라기 보단 projection layer라고 부름  \n",
    "<br></br>\n",
    "- CBOW의 동작 메커니즘  \n",
    "  <p align=\"center\"><img src=https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG></p>\n",
    "  1. projection layer의 크기가 M: 이는 Embedding하고 난 벡터의 차원이 됨\n",
    "  2. 입력층과 투사층 사이의 가중치 W는 V X M 행렬이며, 투사층에서 출력층사이의 가중지 W'는 M X V 행렬\n",
    "   여기서 사용되는 W, W'은 굉장히 작은 Random Value로 이루어짐\n",
    "<br></br>\n",
    "- 주변 단어의 One-hot Vector와 가중치 W행렬의 곱이 이루어지는 과정  \n",
    "  <p align=\"center\"><img src=https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG></p>\n",
    "  위 그림에서 구한 결과 벡터들은 투사층에서 만나 평균인 벡터를 구하게 됨 <- Skip-gram과의 차이점  \n",
    "  <p align=\"center\"><img src=https://wikidocs.net/images/page/22660/word2vec_renew_4.PNG></p>\n",
    "  이렇게 구해진 평균 벡터는 W'과 곱해짐  \n",
    "  이 결과로 One-hot Vector들과 차원이 V로 동일한 벡터가 생성  \n",
    "  이 벡터에 softmax 함수를 취해 score vector를 구함  \n",
    "<br></br>\n",
    "- Score Vector  \n",
    "  Score Vector의 j번째 index가 가진 0과 1 사이의 값은 j번쨰 단어가 중심 단어일 확률을 나타냄  \n",
    "  또한 이 값은 중심 단어 One-hot Vector의 값에 가까워져야 함  \n",
    "  스코어 벡터를 $\\hat{y}$라고 하고 중심 단어를 $y$라고 했을 때 두 벡터값의 오차를 줄이기 위해 CBOW는 loss function으로 cross-entropy 함수를 사용  \n",
    "  <br></br>\n",
    "  $$H(\\hat{y}, y)=-\\Sigma^{|V|}_{j=1}y_jlog(\\hat{y_j})$$\n",
    "  <br></br>\n",
    "  그런데 y가 원-핫 벡터라는 점을 고려하면, 위 식을 다음과 같이 표현할 수 있음  \n",
    "  <br></br>\n",
    "  $$H(\\hat{y}, y) = - y_i log(\\hat{y_i})$$\n",
    "  <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Skip-gram\n",
    "<p align=\"center\"><img src=https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG></p>\n",
    "\n",
    "위 그림과 같이 중심 단어에서 주변 단어를 예측  \n",
    "그림을 보면 이해가 되겠지만 투사층에서 벡터들의 평균을 구하는 과정이 없음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Negative Sampling\n",
    "\n",
    "Word2Vec 모델에는 한 가지 문제점이 존재하는데, 속도의 문제가 있음  \n",
    "출력층의 softmax 함수는 단어 집합 크기의 벡터 내의 모든 값을 0과 1 사이의 값이면서 모두 더하면 1이 되도록 바꾸는 작업을 함  \n",
    "그리고 이에 대한 오차를 구해 모든 단어에 대한 embedding을 조정  \n",
    "Word2Vec은 모든 단어 집합에 대해 softmax 함수를 수행하는데, 이는 주변 단어와 상관없는 모든 단어까지 워드 임베딩 조정 작업을 수행함  \n",
    "그런데 단어 집합의 크기가 수백만에 달하면 이 작업이 매우 무거워짐  \n",
    "<br></br>\n",
    "효율적인 방법으로 주변 단어들과 주변 단어가 아닌 단어 중 일부만 가지고 오고 이진 분류 문제로 변경  \n",
    "즉, 주변 단어는 positive, 나머지는 negative로 두어 이진 분류 문제를 수행  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-4 영어/한국어 Word2Vec 학습시키기</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 영어 Word2Vec 만들기\n",
    "파이썬의 gensim 패키지에는 Word2Vec을 지원하고 있어, gensim 패키지를 이용해 embedding vector로 변환  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 훈련 데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x2bfb6660b20>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 실질적으로 필요한 데이터는 content 사이의 내용  \n",
    "전처리 작업을 통해 xml 문법들은 제거하고, content 내에서도 배경음을 나타내는 단어들을 제거해줄 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\gkwjd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 content 사이의 내용만 가져옴\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 배경음 부분 제거\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해 NLTK를 이용해 문장 토큰화를 수행\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해 구두점을 제거하고, 대문자를 소문자로 변환\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해 NLTK를 이용해 단어 토큰화를 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수: 273380\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수: {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273380\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print(len(result))\n",
    "\n",
    "with open('result.pkl', 'wb') as file:\n",
    "    pickle.dump(result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273380\n"
     ]
    }
   ],
   "source": [
    "with open('result.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)\n",
    "\n",
    "print(len(res))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 3개만 출력\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec의 하이퍼파라미터값\n",
    "  **vector_size** = 워드 벡터의 특징 값. 즉, 입베딩 된 벡터의 차원\n",
    "  **window** = context 윈도우 크기\n",
    "  **min_count** = 단어 최소 빈도 수 제한(빈도가 적은 단어들은 학습하지 않음)\n",
    "  **workers** = 학습을 위한 프로세스 수\n",
    "  **sg** = 0 은 CBOW, 1은 Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8425540924072266), ('guy', 0.8290044069290161), ('boy', 0.7573254704475403), ('gentleman', 0.7433205842971802), ('lady', 0.7432302236557007), ('girl', 0.7315221428871155), ('poet', 0.719691812992096), ('soldier', 0.7045212984085083), ('kid', 0.7008957266807556), ('friend', 0.6702849864959717)]\n"
     ]
    }
   ],
   "source": [
    "# man과 가장 유사한 단어 확인\n",
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Word2Vec 모델 저장하고 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8425540924072266), ('guy', 0.8290044069290161), ('boy', 0.7573254704475403), ('gentleman', 0.7433205842971802), ('lady', 0.7432302236557007), ('girl', 0.7315221428871155), ('poet', 0.719691812992096), ('soldier', 0.7045212984085083), ('kid', 0.7008957266807556), ('friend', 0.6702849864959717)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 한국어 Word2Vec 만들기(네이버 영화 리뷰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x2c02bb551c0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how='any')\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199992\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkwjd\\AppData\\Local\\Temp\\ipykernel_44868\\1818474164.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', \"\")\n"
     ]
    }
   ],
   "source": [
    "# 정규 표현식을 이용해 한글이 아닌 경우 제거\n",
    "train_data['document'] = train_data['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [29:56<00:00, 111.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거, Okt를 사용해 각 문장에 대해 일종의 단어, 형태소 단위로 나누는 토큰화 수행\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# 형태소 분석기 Okt 를 사용한 토큰화 작업\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if word not in stopwords]\n",
    "    tokenized_data.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이: 72\n",
      "리뷰의 평균 길이: 10.716703668146726\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이:', max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이:', sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec으로 토큰화 된 네이버 영화 리뷰 데이터 학습\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 100)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec 임베딩 행렬 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 16477개 단어가 존재하며 각 단어는 100차원으로 구성되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('한석규', 0.8708925843238831), ('안성기', 0.8669458031654358), ('최민수', 0.856795072555542), ('크리스찬', 0.8464598655700684), ('박중훈', 0.8404852747917175), ('이민호', 0.8384249806404114), ('윤제문', 0.8287472128868103), ('이정재', 0.8254367709159851), ('주진모', 0.823716402053833), ('단역', 0.8215270638465881)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('최민식'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('느와르', 0.8695659041404724), ('호러', 0.8642255663871765), ('무협', 0.857586681842804), ('슬래셔', 0.840457022190094), ('물의', 0.8300780057907104), ('무비', 0.8212965130805969), ('물', 0.8118458986282349), ('정통', 0.8094437718391418), ('블랙', 0.8049955368041992), ('멜로', 0.7853151559829712)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('히어로'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 사전 훈련된 Word2Vec 임베딩 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44868\\3938180516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 구글의 사전 훈련된 Word2Vec 모델을 로드.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n\u001b[0m\u001b[0;32m      6\u001b[0m                            filename=\"GoogleNews-vectors-negative300.bin.gz\")\n\u001b[0;32m      7\u001b[0m \u001b[0mword2vec_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드.\n",
    "urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
    "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "404 Error가 발생하는 것으로 보아 사라졌거나 수정된 것으로 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-Embedding Visualization</span>\n",
    "  \n",
    "논문: https://arxiv.org/pdf/1611.05469v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 워드 임베딩 모델로부터 2개의 tsv 파일 생성하기\n",
    "\n",
    "편의를 위해 이전 챕터에서 학습하고 저장했던 Word2Vec 모델인 'eng_w2v'를 재사용  \n",
    "\n",
    "```terminal\n",
    "!python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 18:11:01,466 - word2vec2tensor - INFO - running c:\\Users\\gkwjd\\anaconda3\\lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input eng_w2v --output eng_w2v\n",
      "2024-01-24 18:11:01,466 - keyedvectors - INFO - loading projection weights from eng_w2v\n",
      "2024-01-24 18:11:02,828 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (21613, 100) matrix of type float32 from eng_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-01-24T18:11:02.807638', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n",
      "2024-01-24 18:11:04,862 - word2vec2tensor - INFO - 2D tensor file saved to eng_w2v_tensor.tsv\n",
      "2024-01-24 18:11:04,863 - word2vec2tensor - INFO - Tensor metadata file saved to eng_w2v_metadata.tsv\n",
      "2024-01-24 18:11:04,864 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 파일이 생성되면 성공\n",
    "<p align=\"center\"><img src=https://wikidocs.net/images/page/50704/eng_w2v.PNG></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 임베딩 프로젝터를 사용해 시각화 하기\n",
    "\n",
    "링크 : https://projector.tensorflow.org/\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-6 스탠프드 대학교의 Glove</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기존 방법론에 대한 비판\n",
    "\n",
    "- LSA  \n",
    "  각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소하여 잠재된 의미를 끌어내는 방법론  \n",
    "\n",
    "  장점: 카운틑 기반으로 코퍼스의 전체적인 통계 정보를 고려  \n",
    "  단점: 유추 작업에서 성능이 떨어짐\n",
    "\n",
    "- Word2Vec\n",
    "  실제값과 예측값에 대한 오차를 Loss Function을 통해 줄여나가며 학습하는 예측 기반의 방법론  \n",
    "\n",
    "  장점: 단어 간 유추 작업에는 LSA보다 뛰어남\n",
    "  단점: 윈도우 크기 내에서만 주변 단어를 고려하므로 코퍼스 전체적인 통계 정보를 반영하지 못함\n",
    "\n",
    "- Glove  \n",
    "  위 한계들을 보완하기 위해 카운트 기반 방법과 예측 기반 방법론 두 가지 모두 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 윈도우 기반 동시 등장 행렬\n",
    "\n",
    "단어의 동시 등장 행렬: 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬  \n",
    "  \n",
    "[예제]  \n",
    "> I like deep learning  \n",
    "> I like NLP  \n",
    "> I enjoy flying  \n",
    "\n",
    "윈도우 크기가 N일 때는 좌, 우에 존재하는 N개의 단어만 참고  \n",
    "<br></br>\n",
    "윈도우 크기가 1일 때 동시 등장 행렬은 다음과 같음  \n",
    "\n",
    "|카운트|I|like|enjoy|deep|learing|NLP|flying|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|I|0|2|1|0|0|0|0|\n",
    "|like|2|0|0|1|0|1|0|\n",
    "|enjoy|1|0|0|0|0|0|1|\n",
    "|deep|0|1|0|0|1|0|0|\n",
    "|learning|0|0|0|1|0|0|0|\n",
    "|NLP|0|1|0|0|0|0|0|\n",
    "|flying|0|0|1|0|0|0|0|\n",
    "\n",
    "\n",
    "위 행렬은 Transpose해도 동일한 행렬이 됨  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 동시 등장 확률\n",
    "\n",
    "- 동시 등장 확률  \n",
    "  $P(k|i)$  \n",
    "  i는 중심 단어(Center Word), k는 주변 단어(Context Word)  \n",
    "  \n",
    "  동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률  \n",
    "\n",
    "  동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모  \n",
    "  i행 k열의 값을 분자로 한 값  \n",
    "\n",
    "  [예시 - 동시 등장 확률과 크기 관계 비]\n",
    "\n",
    "  ||k=solid|k=gas|k=water|k=fashion|\n",
    "  |---|---|---|---|---|\n",
    "  |P(k l ice)|0.00019 | 0.000066 | 0.003 | 0.000017 |\n",
    "  |P(k l steam) | 0.000022 | 0.00078 | 0.0022 | 0.000018 |\n",
    "  |P(k l ice) / P(k l steam) | 8.9 | 0.085 | 1.36 | 0.96 |\n",
    "\n",
    "  위 표를 통해 할 수 있는 사실은 ice가 등장했을 때 solid가 등장할 확률 0.0019는. steam이 등장했을 때 solid가 등장할 확률인 0.000022 보다 약 8.9배 크다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Function\n",
    "\n",
    "- 용어 정리  \n",
    "  - $X$: 동시 등장 행렬(Co-occurrence Matrix)  \n",
    "  - $X_{ij}$: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수  \n",
    "  - $X_i$: $\\Sigma_jX_{ij}$: 동시 등장 행렬에서 i행의 값을 모두 더한 값  \n",
    "  - $P_{ik}$: $P(k|i)$=$\\frac{X_{ik}}{X_i}$: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률  \n",
    "  - $\\frac{p_{ik}}{p_{jk}}$  \n",
    "  - $w_i$: 중심 단어 i의 임베딩 벡터  \n",
    "  - $\\hat{w_k}$: 주변 단어 k의 임베딩 벡터  \n",
    "\n",
    "- GloVe의 아이디어  \n",
    "  임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것  \n",
    "  즉, 이를 만족하도록 임베딩 벡터를 만드는 것이 목표  \n",
    "  $$dot product(w_i, \\hat{w_k})\\approx P(k|i)=P_{ik}$$  \n",
    "  더 정확하게는  \n",
    "  $$dot product(w_i, \\hat{w_k})\\approx P(k|i)=log P(k|i) = log P_{ik}$$  \n",
    "\n",
    "- Loss Function 설계\n",
    "  $$F(w_i, w_j, \\hat{w_k})=\\frac{P_{ik}}{P_{jk}}$$    \n",
    "  위 식에서 시작이 되며 F가 어떤 식인지는 아직 정해지지 않음  \n",
    "\n",
    "  함수 F는 두 단어 사이의 동시 등장 확률의 크기 관계 비 정보를 벡터 공간에 인코딩하는 것이 목적  \n",
    "  이를 위해 $w_i$와 $w_j$라는 두 벡터 차이를 함수 F의 입력으로 사용하는 것을 제안  \n",
    "  $$F(w_i-w_j, \\hat{w_k})=\\frac{P_{ik}}{P_{jk}}$$  \n",
    "    \n",
    "  그런데 위 식에서 우변은 스칼라값이고, 좌변은 벡터값이므로  \n",
    "  F의 두 입력에 Dot Product를 수행  \n",
    "  $$F((w_i-w_j)^T\\hat{w_k})=\\frac{P_{ik}}{P_{jk}}$$ \n",
    "\n",
    "  그런데 여기서 중심 단어 $w$와 주변 단어 $\\hat{w}$는 무작위 선택이므로 이 둘의 관계가 자유롭게 교환될 수 있도록 해야함  \n",
    "  이를 위해 함수 $F$가 실수의 덧셈과 양수의 곱셈에 대해 준동형(Homomorphism)을 만족해야함  \n",
    "  즉, $F(a+b) = F(a)F(b)$ 를 만족해야함   \n",
    "\n",
    "  이제 위 준동형식을 GloVe식에 적용할 수 있도록 조금씩 바꿔보자  \n",
    "  $$F(v_1^Tv_2+v_3^Tv_4)=F(v_1^Tv_2)F(v_3^Tv_4)$$  \n",
    "  그런데 GloVe 식에서는 두 벡터의 차이를 함수 F의 입력으로 받았는데 GloVe식에 바로 적용을 위해 준동형식을 뺄셈에 대한 준동형식으로 변경하면  \n",
    "  $$F(v_1^Tv_2-v_3^Tv_4)=\\frac{F(v_1^Tv_2)}{v_3^Tv_4}$$  \n",
    "\n",
    "  이제 이 준동형식을 GloVe에 적용해보자  \n",
    "  $$F((w_i-w_j)^T\\hat{w_k})=\\frac{F(w_i^T\\hat{w_k})}{F(w_j^T\\hat{w_k})}$$  \n",
    "  $$\\frac{P_{ik}}{P_{jk}}=\\frac{F(w_i^T\\hat{w_k})}{F(w_j^T\\hat{w_k})}$$\n",
    "  $$F(w_i^T\\hat{w_k})=P_{ik}=\\frac{X_{ik}}{X_i}$$  \n",
    "\n",
    "  이제 이를 만족하는 함수 F를 찾아야 하는데 이를 정확히 만족시키는 함수가 지수 함수  \n",
    "  따라서 F를 지수함수 exp라고 해보자  \n",
    "  $$exp(w_i^T\\hat{w_k}-w_j^T\\hat{w_k})=\\frac{exp(w_i^T\\hat{w_k})}{exp(w_j^T\\hat{w_k})}$$\n",
    "  $$exp(w_i^T\\hat{w_k})=P{ik}=\\frac{X_{ik}}{X_i}$$  \n",
    "\n",
    "  위의 두 번째 식으로부터 다음과 같은 식을 얻을 수 있음  \n",
    "  $$w_i^T\\hat{w_k}=log P{ik}=log(\\frac{X_{ik}}{X_i})=log X_{ik}-log X_i$$  \n",
    "\n",
    "  그런데 여기서 $w_i, \\hat{w_k}$는 두 값의 위치를 바꿔도 식이 성립해야 하는데 $log X_i$항이 걸림  \n",
    "  따라서 이를 $w_i$에 대한 편향 $b_i$라는 상수항으로 대체  \n",
    "  같은 이유로 $\\hat{k}$에 대한 편향 $\\hat{b_k}$를 추가  \n",
    "  $$w_i^T\\hat{w_k}+b_i+\\hat{b_k}=log X_{ik}$$  \n",
    "\n",
    "  이를 우변의 값과의 차이를 최소화하는 방향으로 좌변의 4개의 항은 학습을 통해 값이 바뀌는 변수가 됨  \n",
    "  따라서 Loss Function은 다음과 같이 일반화할 수 있음  \n",
    "  $$Loss\\,function=\\Sigma^V_{m,n=1}(w_m^T\\hat{w_n}+b_m+\\hat{b_n}-logX_{mn})^2$$\n",
    "\n",
    "  또한 여기서 $X_{ik}$값이 0이 될 수 있는데 이에 대한 대안중 하나로 $logX_{ik}$항을 $log(1+X_{ik})$로 변경하는 것  \n",
    "  하지만 이렇게 하더라도 동시 등장 행렬 $X$는 DTM처럼 희소 행렬일 가능성이 다분함  \n",
    "  즉, 동시 등장 빈도가 적어서 많은 값이 작은 수치를 가지거나 0일 경우가 있는데 $X_{ik}$가 굉장히 낮은 경우 정보에 도움이 되지 않는다고 판단해 이에 대한 가중치를 고민하였고  \n",
    "  $X_{ik}$의 값에 영향을 받는 weighting function $f(X_{ik})$를 손실 함수에 도입  \n",
    "\n",
    "  다음은 GloVe에 도입되는 $f(X_{ik})$그래프이다  \n",
    "  <p align=\"center\"><img src=https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG></p>  \n",
    "\n",
    "  $$f(x)=min(1,(x/x_{max})^{3/4})$$  \n",
    "\n",
    "  최종적으로 Loss Function은 다음과 같다  \n",
    "  $$Loss\\,function=\\Sigma^V_{m,n=1}f(X_{mn})(w_m^T\\hat{w_n}+b_m+\\hat{b_n}-logX_{mn})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GloVe 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement glove-python-binary (from versions: none)\n",
      "ERROR: No matching distribution found for glove-python-binary\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\gkwjd\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install glove-python-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬 버전이 3.6~3.8에서만 된다고 하여 실습은 생략함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-7 내부 단어를 고려하는 FastText</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 내부 단어(subword)의 학습\n",
    "\n",
    "FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급  \n",
    "n을 몇으로 결정하는지에 따라 단어들이 얼마나 분리되는지 결정됨  \n",
    "\n",
    "[예시]  \n",
    "n = 3, apple => <ap, app, ppl, ple, le>로 분리하고 이들을 벡터로 만들어냄  \n",
    "\n",
    "또한 여기에 추가적으로 하나를 더 벡터화 하는데, 기존 단어에 <>를 붙인 토큰  \n",
    "\n",
    "즉, n = 3인 경우  <ap, app, ppl, ple, le>, <apple> 6개의 토큰을 벡터화  \n",
    "\n",
    "그런데 실제로 사용할 때는 n의 최소값과 최대값으로 범위를 설정할 수 있음  \n",
    "=> Word2Vec에서 얻을 수 없었던 강점을 가지게 됨  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모르는 단어(Out of Vocabulary)에 대한 대응  \n",
    "FastText의 인공신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해 워드 임베딩이 됨  \n",
    "이렇게 되면 장점은 <span style=\"background-color:#fff5b1\">데이터 셋만 충분하다면 위와 같은 내부 단어(Subword)를 통해 모르는 단어(Out of Vocabulary)에 대해서도 다른 단어와의 유사도를 계산할 수 있음</span>\n",
    "\n",
    "[예시]  \n",
    "birthplace란 단어를 학습하지 않은 상태라고 했을 때 다른 단어에서 birth와 place라는 내부 단어가 있었다면, FastText는 birthplace의 벡터를 얻을 수 있음  \n",
    "이는 모르는 단어에 제대로 대처할 수 없는 Word2Vec과 GloVe와는 다른 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 단어 집합 내 빈도 수가 적었던 단어에 대한 대응  \n",
    "단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우라면, Word2Vec과 비교해 비교적 높은 임베딩 벡터값을 얻을 수 있음  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 실습으로 비교하는 Word2Vec VS FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44868\\953364053.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"eng_w2v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'electrofishing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    771\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gkwjd\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'electrofishing' not present\""
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"eng_w2v\")\n",
    "model.most_similar('electrofishing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolux', 0.855473518371582),\n",
       " ('electrolyte', 0.8533106446266174),\n",
       " ('electroencephalogram', 0.8485924005508423),\n",
       " ('electroshock', 0.8472487330436707),\n",
       " ('electro', 0.8454289436340332),\n",
       " ('electrochemical', 0.8294127583503723),\n",
       " ('electrogram', 0.825486958026886),\n",
       " ('electronic', 0.8231189846992493),\n",
       " ('electron', 0.8159002065658569),\n",
       " ('electrons', 0.8116163611412048)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('electrofishing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:skyblue\">12-9 Pytorch의 nn.Embedding()</span>\n",
    "\n",
    "1. Embedding layer를 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 ㅏㅇ법  \n",
    "2. pre-trained word embedding들을 가져와 사용하는 방법  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 임베딩 층은 룩업 테이블이다\n",
    "---\n",
    "\n",
    "임베딩 층의 입력으로 사용되기 위해서 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어 있어야 함  \n",
    "어떤 단어 -> 단어에 부여된 고유한 정수값 -> 임베딩 층 통과 -> 밀집 벡터  \n",
    "\n",
    "임베딩 층은 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됨  \n",
    "\n",
    "- 정수를 밀집 벡터(임베딩 벡터)로 맵핑  \n",
    "  특정 단어와 맵핑되는 정수를 인덱스로 가지는 테이블로부터 임베딩 벡터 값을 가져오는 룩업 테이블  \n",
    "  그리고 이 테이블은 단어 집합의 크기만큼의 행을 가지므로 모든 단어는 고유한 임베딩 벡터를 가짐  \n",
    "  <p align=\"center\"><img src=https://wikidocs.net/images/page/33793/lookup_table.PNG></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 룩업 테이블 과정을 nn.Embedding()을 사용하지 않고 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 2, 'how': 3, 'you': 4, 'know': 5, 'code': 6, 'need': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data = \"you need to know how to code\"\n",
    "\n",
    "# 중복을 제거한 단어들의 집합인 단어 집합 생성\n",
    "word_set = set(train_data.split())\n",
    "\n",
    "# 단어 집합의 각 단어에 고유한 정수 맵핑\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44868\\1552936283.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 단어 집합의 크기를 행으로 가지는 임베딩 테이블 구현\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 임베딩 벡터의 차원은 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m embedding_table = torch.FloatTensor([\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 단어 집합의 크기를 행으로 가지는 임베딩 테이블 구현\n",
    "# 임베딩 벡터의 차원은 3\n",
    "embedding_table = torch.FloatTensor([\n",
    "    [0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0],\n",
    "    [0.2, 0.9, 0.3],\n",
    "    [0.1, 0.5, 0.7],\n",
    "    [0.2, 0.1, 0.8],\n",
    "    [0.4, 0.1, 0.1],\n",
    "    [0.1, 0.8, 0.9],\n",
    "    [0.6, 0.1, 0.1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
