{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-1 단어의 표현 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 단어의 표현 방법\n",
    "국소 표현(Local Representation): 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법 \n",
    "> puppy, cute, lovely라는 단어 각각에 1번, 2번 3번 등과 같은 숫자를 mapping하여 부여  \n",
    " \n",
    "분산 표현(Distributed Representation): 그 단어를 표현하고자 주변을 참고하여 단어를 표현\n",
    "> puppy라는 단어 근처에는 주로 cute, lovely라는 단어가 자주 등장하므로, puppy라는 단어는 cute, lovely한 느낌이다로 단어 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 단어 표현의 카테고리화\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://wikidocs.net/images/page/31767/wordrepresentation.PNG\" >\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-1 Bag of Words\n",
    "\n",
    "**Bag of Words**: 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "> [과정]  \n",
    "> (1) 각 단어에 고유한 정수 인덱스를 부여  \n",
    "> (2) 각 index의 위치에 단어 트큰의 등장 횟수를 기록한 벡터를 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**문서 1: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.**  \n",
    "\n",
    "위 문서 1에 대해 BoW를 만들어봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "    # 온점 제거 및 형태소 분석\n",
    "    document = document.replace('.', '')\n",
    "    tokenized_document = okt.morphs(document)\n",
    "\n",
    "    word_to_index = {}\n",
    "    bow = []\n",
    "\n",
    "    for word in tokenized_document:\n",
    "        if word not in word_to_index.keys():\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            # BoW에 전부 기본값 1을 넣음\n",
    "            bow.insert(len(word_to_index)-1, 1)\n",
    "        else:\n",
    "            # 재등장하는 단어의 index\n",
    "            index = word_to_index[word]\n",
    "            bow[index] += 1\n",
    "\n",
    "    return word_to_index, bow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector: [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary:', vocab)\n",
    "print('bag of words vector:', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BoW의 다른 예제들\n",
    "\n",
    "**문서 2: 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "bag of words vector : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "doc2 = '소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CountVectorizer 클래스로 BoW 만들기\n",
    "\n",
    "Scikit-Learn에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원  \n",
    "(영어에서 손쉽게 BoW를 만들 수 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector: [[1 1 2 1 2 1]]\n",
      "vocabulary: {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print('bag of words vector:', vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어의 index가 어떻게 부여되었는지 출력\n",
    "print('vocabulary:', vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2 이상인 문자에 대해서만 토큰으로 인식하기 때문에 사라짐  \n",
    "\n",
    "- 주의  \n",
    "  CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 잘라 한국어에서 사용하기 힘들다  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 불용어를 제거한 BoW 만들기\n",
    "\n",
    "CountVectorizer 는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 사용자가 직접 정의한 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector: [[1 1 1 1 1]]\n",
      "vocabulary: {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "\n",
    "print('bag of words vector:', vect.fit_transform(text).toarray())\n",
    "print('vocabulary:', vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) CountVectorizer에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector: [[1 1 1]]\n",
      "vocabulary: {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "print('bag of words vector:', vect.fit_transform(text).toarray())\n",
    "print('vocabulary:', vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) NLTK에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 1 1]]\n",
      "vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "print('bag of words vector :',vect.fit_transform(text).toarray()) \n",
    "print('vocabulary :',vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-3 문서의 벡터화: 문서 단어 행렬(Document-Term Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 단어 행렬(Document-Term Matrix, DTM): 서로 다른 문서들의 BoW들을 결합한 표현 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 문서 단어 행렬의 표기법\n",
    "\n",
    "문서 단어 행렬: 다수의 문서에 등장하는 각 단어들의 빈도를 행렬로 표현한 것  \n",
    "즉, 각 문서에 대한 BoW를 하나의 행렬로 만든 것  \n",
    "\n",
    ">[예시]  \n",
    ">문서1 : 먹고 싶은 사과  \n",
    ">문서2 : 먹고 싶은 바나나  \n",
    ">문서3 : 길고 노란 바나나 바나나  \n",
    ">문서4 : 저는 과일이 좋아요  \n",
    "\n",
    "띄어쓰기 단위 tokenization이라 가정하면\n",
    "||과일이|길고|노란|먹고|바나나|사과|싶은|저는|좋아요|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|문서1|0|0|0|1|0|1|1|0|0|\n",
    "|문서2|0|0|0|1|1|0|1|0|0|\n",
    "|문서3|0|1|1|0|2|0|0|0|0|\n",
    "|문서4|1|0|0|0|0|0|0|1|1|\n",
    "\n",
    "==> 서로 비교할 수 있도록 수치화 할 수 있다는 점에서 의의를 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 문서 단어 행렬의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Sparse Representation\n",
    "DTM은 각 행을 문서 벡터라 가정하면 각 문서의 차원은 전체 단어 집합의 크기를 가짐  \n",
    "즉, corpus가 방대한 데이터라면 문서 벡터의 차원이 매우 커짐  \n",
    "또한 많은 문서의 대부분 값은 0을 가질 것이고 원-핫 벡터와 같이 공간적 낭비, 계산 리소스 증가의 단점\n",
    "\n",
    "이 대부분 값이 0인 표현을 sparse vector라고 하는데 이는 많은 양의 저장 공간과 높은 계산 복잡도를 요구하므로 전처리를 통해 단어 집합의 크기를 줄이는 일이 중요  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 단순 빈도 수 기반 접근\n",
    "모든 단어에 대해 빈도 표기를 하는 방법은 다음의 한계를 가짐\n",
    "\n",
    "불용어 the의 경우 어떤 문서이든 자주 등장하는데 문서1, 문서2, 문서3에서 동일하고 the 빈도수가 높다고 해서 이 문서들이 유사한 문서라고 판단해서는 안됨  \n",
    "따라서 DTM에서 불용어와 중요한 단어에 대해 가중치를 줄 수 있는 TF-IDF를 다음에 소개함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-4 TF-IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TF-IDF\n",
    "\n",
    "TF-IDF: 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용해 DTM내의 각 단어들마다 중요한 정도를 가중치로 주는 방법  \n",
    "우선 DTM 생성 -> TF-IDF 가중치 부여  \n",
    "\n",
    "$$TF-IDF=TF \\times IDF$$\n",
    "\n",
    "$d: 문서, t: 단어, n: 문서 총 개수$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) tf(d, t): 특정 문서 d에서의 특정 단어 t의 등장 횟수\n",
    "DTM에서 각 단어들이 가진 값들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) df(t): 특정 단어 t가 등장한 문서의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) idf(t): df(t)에 반비례하는 수\n",
    "\n",
    "$$idf(t)=log(\\frac{n}{1+df(t)})$$\n",
    "\n",
    "[**log를 사용하는 이유 1**]  \n",
    "문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지게 되는데 이를 방지하기 위해\n",
    "\n",
    "[예시]\n",
    "n = 1,000,000일 때, log의 밑은 10을 사용  \n",
    "|단어$t$|$df(t)$|$idf(d,t)$|\n",
    "|---|---|---|\n",
    "|word1|1|6|\n",
    "|word2|100|4|\n",
    "|word3|1,000|3|\n",
    "|word4|10,000|2|\n",
    "|word5|100,000|1|\n",
    "|word6|1,000,000|0|\n",
    "\n",
    "log를 사용하지 않았을 때\n",
    "$idf(t)=n/df(t)$\n",
    "|단어$t$|$df(t)$|$idf(t)$|\n",
    "|---|---|---|\n",
    "|word1|1|1,000,000|\n",
    "|word2|100|10,000|\n",
    "|word3|1,000|1,000|\n",
    "|word4|10,000|100|\n",
    "|word5|100,000|10|\n",
    "|word6|1,000,000|1|\n",
    "\n",
    "[**log를 사용하는 이유 2**]  \n",
    "불용어 등 자주 쓰이는 단어들은 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장  \n",
    "이는 수백 배 더 자주 등장할 수도 있는데, 이런 경우 log를 씌어주지 않으면, 희귀 단어들에  \n",
    "엄청난 가중치가 부여됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**  \n",
    "TF-IDF는 모든 문서에 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단  \n",
    "\n",
    "[예시]  \n",
    "||과일이|길고|노란|먹고|바나나|사과|싶은|저는|좋아요|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|문서1|0|0|0|1|0|1|1|0|0|\n",
    "|문서2|0|0|0|1|1|0|1|0|0|\n",
    "|문서3|0|1|1|0|2|0|0|0|0|\n",
    "|문서4|1|0|0|0|0|0|0|1|1|\n",
    "\n",
    "TF는 앞서 사용한 DTM을 그대로 사용하면, 그것이 각 문서에서의 각 단어의 TF가 됨  \n",
    "이제는 IDF를 구해야 하는데, 로그는 자연 로그를 사용하여 계산  \n",
    "\n",
    "|단어|IDF(역 문서 빈도)|\n",
    "|---|---|\n",
    "|과일이|ln(4/(1+1))=0.693147|\n",
    "|길고|ln(4/(1+1))=0.693147|\n",
    "|노란|ln(4/(1+1))=0.693147|\n",
    "|먹고|ln(4/(2+1))=0.287682|\n",
    "|바나나|ln(4/(2+1))=0.287682|\n",
    "|사과|ln(4/(1+1))=0.693147|\n",
    "|싶은|ln(4/(2+1))=0.287682|\n",
    "|저는|ln(4/(1+1))=0.693147|\n",
    "|좋아요|ln(4/(1+1))=0.693147|\n",
    "\n",
    "문서의 총 수는 4이므로 분자는 항상 4로 동일함  \n",
    "TF-IDF는 위에서 구한 TF값과 IDF를 곱해 구하면 되므로 생략함  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파이썬으로 TF-IDF 직접 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log    # IDF 계산을 위해\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs)       # 총 문서의 수\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF를 구함(DTM을 데이터프레임에 저장해 출력)\n",
    "result = []\n",
    "\n",
    "# 각 문서에 대해 아래 연산을 반복\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[i].append(tf(t,d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns=vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어에 대한 IDF 값 구하기\n",
    "result = []\n",
    "\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=['IDF'])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬 출력하기\n",
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[i].append(tfidf(t, d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns=vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 사이킷런을 이용한 DTM과 TF-IDF 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어와 mapping된 index 출력\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런은  TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-5 코사인 유사도를 이용한 추천 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cosine Similarity\n",
    "두 벡터 간의 코사인 각도를 이용해 구할 수 있는 두 벡터의 유사도를 의미(-1 ~ 1)  \n",
    "> 1 : 벡터 방향이 완전히 동일  \n",
    "> 0 : 90도의 각을 이룸  \n",
    "> -1: 180도로 완전히 반대의 방향을 가짐  \n",
    "\n",
    "<p align=\"center\"><img src=https://wikidocs.net/images/page/24603/%EC%BD%94%EC%82%AC%EC%9D%B8%EC%9C%A0%EC%82%AC%EB%8F%84.PNG></p>\n",
    "\n",
    "코사인 유사도의 식은 다음과 같음  \n",
    "$$similarity=cos(\\Theta)=\\frac{A\\cdot B}{||A||||B||}=\\frac{\\Sigma^n_{i=1}A_i\\times B_i}{\\sqrt{\\Sigma^n_{i=1}(A_i)^2}\\times\\sqrt{\\Sigma^n_{i=1}(B_i)^2}}$$\n",
    "\n",
    "[예시]  \n",
    "문서1 : 저는 사과 좋아요  \n",
    "문서2 : 저는 바나나 좋아요  \n",
    "문서3 : 저는 바나나 좋아요 저는 바나나 좋아요  \n",
    "  \n",
    "띄어쓰기 기준 tokenizatioin이라 가정하고, 세 문서에 대해 DTM은 다음과 같음  \n",
    "||바나나|사과|저는|좋아요|\n",
    "|---|---|---|---|---|\n",
    "|문서1|0|1|1|1|\n",
    "|문서2|1|0|1|1|\n",
    "|문서3|2|0|2|2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numpy를 사용해 코사인 유사도를 계산하는 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1, 문서2 유사도: 0.6666666666666667\n",
      "문서1, 문서3 유사도: 0.6666666666666667\n",
      "문서2, 문서3 유사도: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "docs = [\"저는 사과 좋아요\",\n",
    "        \"저는 바나나 좋아요\",\n",
    "        \"저는 바나나 좋아요 저는 바나나 좋아요\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "DTM = vectorizer.fit_transform(docs).toarray()\n",
    "\n",
    "doc1, doc2, doc3 = DTM[0], DTM[1], DTM[2]\n",
    "\n",
    "print('문서1, 문서2 유사도:', cos_sim(doc1, doc2))\n",
    "print('문서1, 문서3 유사도:', cos_sim(doc1, doc3))\n",
    "print('문서2, 문서3 유사도:', cos_sim(doc2, doc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 유사도를 이용한 추천 시스템 구현하기\n",
    "캐글에서 사용되었던 영화 Dataset을 가지고 영화 추천 시스템 만들기  \n",
    "TF-IDF, Cosine-Similarity 만으로 영화 줄거리에 기반해 영화를 추천하는 시스탬  \n",
    "\n",
    "[Dataset]  \n",
    "https://www.kaggle.com/rounakbanik/the-movies-dataset   \n",
    "  \n",
    "총 24개의 열을 가진 45,466개의 sample로 구성된 영화 정보 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkwjd\\AppData\\Local\\Temp\\ipykernel_37916\\3967704491.py:5: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"movies_metadata.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "\n",
       "                               homepage    id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story   862  tt0114709                en   \n",
       "1                                   NaN  8844  tt0113497                en   \n",
       "\n",
       "  original_title                                           overview  ...  \\\n",
       "0      Toy Story  Led by Woody, Andy's toys live happily in his ...  ...   \n",
       "1        Jumanji  When siblings Judy and Peter discover an encha...  ...   \n",
       "\n",
       "  release_date      revenue runtime  \\\n",
       "0   1995-10-30  373554033.0    81.0   \n",
       "1   1995-12-15  262797249.0   104.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                                     tagline      title  video vote_average  \\\n",
       "0                                        NaN  Toy Story  False          7.7   \n",
       "1  Roll the dice and unleash the excitement!    Jumanji  False          6.9   \n",
       "\n",
       "  vote_count  \n",
       "0     5415.0  \n",
       "1     2413.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = pd.read_csv(\"movies_metadata.csv\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 2만개의 sample을 data에 저장\n",
    "data = data.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overview 열의 결측값 수: 135\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF를 연산할 때 데이터에 Null 값이 들어있으면 에러가 발생하므로 Null값 확인\n",
    "# overview 열에 존재하는 모든 결측값을 전부 카운트해 출력\n",
    "print('overview 열의 결측값 수:', data['overview'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null대신 빈 값으로 대체\n",
    "data['overview'] = data['overview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 행렬의 크기(shape): (20000, 47487)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 행렬 구하기\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['overview'])\n",
    "print('TF-IDF 행렬의 크기(shape):', tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코사인 유사도 연산 결과: (20000, 20000)\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print('코사인 유사도 연산 결과:', cosine_sim.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20000 X 20000 행렬이 나오는데 이는 20000개의 각 문서 벡터와 자기 자신을 포함한 20000개의 문서 벡터간의 유사도가 기록된 행렬  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 영화의 타이틀을 key, 영화의 index를 value로 하는 딕셔너리 생성\n",
    "title_to_index = dict(zip(data['title'], data.index))\n",
    "\n",
    "# 영화 제목 Father fo the Brige Part II의 index 리턴\n",
    "idx = title_to_index['Father of the Bride Part II']\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택한 영화의 제목을 입력해 코사인 유사도를 통해 가장 overview가 유사한 10개의 영화를 찾아내는 함수\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # 선택한 영화의 타이틀로부터 해당 영화의 index를 받아옴\n",
    "    idx = title_to_index[title]\n",
    "\n",
    "    # 해당 영화와 모든 영화의 유사도를 가져옴\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # 유사도에 따라 영화들을 정렬\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 가장 유사한 10개의 영화를 받아옴\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 index를 얻음\n",
    "    movie_indices = [idx[0] for idx in sim_scores]\n",
    "\n",
    "    # 가장 유사한 10개의 영화 제목을 return\n",
    "    return data['title'].loc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12481                            The Dark Knight\n",
       "150                               Batman Forever\n",
       "1328                              Batman Returns\n",
       "15511                 Batman: Under the Red Hood\n",
       "585                                       Batman\n",
       "9230          Batman Beyond: Return of the Joker\n",
       "18035                           Batman: Year One\n",
       "19792    Batman: The Dark Knight Returns, Part 1\n",
       "3095                Batman: Mask of the Phantasm\n",
       "10122                              Batman Begins\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-6 단어와 문서의 유사도를 구하는 다양한 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Euclidean distance\n",
    "다차원 공간에서 두 개의 점 $p$, $q$가 각각 $p=(p_1, p_2, p_3, ..., p_n)$과 $q=(q_1, q_2, q-3, ..., q_n)$의 좌표를 가질 때 두 점 사이의 거리를 계산하는 유클리드 공식은 다음과 같음  \n",
    "$$\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+...+(q_n-p_n)^2}=\\sqrt{\\Sigma^n_{i=1}(q_i-p_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[예시]  \n",
    "||바나나|사과|저는|좋아요|\n",
    "|---|---|---|---|---|\n",
    "|문서1|2|3|0|1|\n",
    "|문서2|1|2|3|1|\n",
    "|문서3|2|1|2|2|\n",
    "\n",
    "단어의 개수가 4개이므로, 4차원 공간에 문서1, 문서2, 문서3을 배치하는 것과 동일  \n",
    "이 때 다음과 같은 문서 Q에 대해 문서1, 문서2, 문서3 중 가장 유사한 문서를 찾아내고자 함  \n",
    "\n",
    "||바나나|사과|저는|좋아요|\n",
    "|---|---|---|---|---|\n",
    "|문서Q|1|1|0|1|\n",
    "\n",
    "이 때 Euclidean Distance를 통해 휴사도를 구하려고 한다면, 문서 Q또한 다른 문서들처럼 4차원 공간에 배치시켰다는 관점에서 구해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1과 문서Q의 거리: 2.23606797749979\n",
      "문서2와 문서Q의 거리: 3.1622776601683795\n",
      "문서3과 문서Q의 거리: 2.449489742783178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dist(x, y):\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "doc1 = np.array((2, 3, 0, 1))\n",
    "doc2 = np.array((1, 2, 3, 1))\n",
    "doc3 = np.array((2, 1, 2, 2))\n",
    "docQ = np.array((1, 1, 0, 1))\n",
    "\n",
    "print('문서1과 문서Q의 거리:', dist(doc1, docQ))\n",
    "print('문서2와 문서Q의 거리:', dist(doc2, docQ))\n",
    "print('문서3과 문서Q의 거리:', dist(doc3, docQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서1이 문서Q와 가장 유사하다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jaccard Similarity\n",
    "\n",
    "A와 B 두 개의 집합이 있다고 가정했을 때  \n",
    "합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있음 -> Jaccard Similarity  \n",
    "> 1 : 두 집합이 동일함  \n",
    "> 2 : 공통 원소가 없음\n",
    "\n",
    "$$J(A,B)=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1 : ['apple', 'banana', 'everyone', 'like', 'likey', 'watch', 'card', 'holder']\n",
      "문서2 : ['apple', 'banana', 'coupon', 'passport', 'love', 'you']\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"apple banana everyone like likey watch card holder\"\n",
    "doc2 = \"apple banana coupon passport love you\"\n",
    "\n",
    "# 토큰화\n",
    "tokenized_doc1 = doc1.split()\n",
    "tokenized_doc2 = doc2.split()\n",
    "\n",
    "print('문서1 :',tokenized_doc1)\n",
    "print('문서2 :',tokenized_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1과 문서2의 합집합: {'likey', 'love', 'like', 'holder', 'coupon', 'card', 'passport', 'you', 'watch', 'everyone', 'banana', 'apple'}\n"
     ]
    }
   ],
   "source": [
    "# 문서1과 문서2의 합집합\n",
    "union = set(tokenized_doc1).union(set(tokenized_doc2))\n",
    "print('문서1과 문서2의 합집합:', union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서1과 문서2의 교집합: {'banana', 'apple'}\n"
     ]
    }
   ],
   "source": [
    "# 문서1과 문서2의 교집합\n",
    "intersection = set(tokenized_doc1).intersection(set(tokenized_doc2))\n",
    "print('문서1과 문서2의 교집합:', intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자카드 유사도: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('자카드 유사도:', len(intersection)/len(union))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
