{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPN76XD1CAkv3PWBR6gR6N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 10-1 Language Model\n","언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당하는 모델\n","\n","- 통계를 이용한 방법\n","- 인공 신경망을 이용한 방법(GPT, BERT)"],"metadata":{"id":"2PL02vTbo93a"}},{"cell_type":"markdown","source":["## 1. Language Model\n","Language Model(LM): 단어 시퀀스에 확률을 할당.\n","> - 이전 단어들이 주어졌을 때 다음 단어를 예측하도록(보편적)\n","- 양쪽의 단어들로부터 가운데 비어있는 단어를 예측하는 언어 모델(BERT)"],"metadata":{"id":"l5NZetzOpODy"}},{"cell_type":"markdown","source":["## 2. 주어진 이전 단어들로부터 다음 단어 예측하기\n","### A. 단어 시퀀스의 확률\n","하나의 단어를 $w$, 단어 시퀀스를 대문자 $W$라고 한다면, $n$개의 단어가 등장하는 단어 시퀀스 $W$의 확률은  \n","$$P(W)=P(w_1, w_2, w_3, w_4, w_5, ..., w_n)$$\n","\n","### B. 다음 단어 등장 확률\n","$n-1$개의 단어가 나열된 상태에서 $n$번째 단어의 확률은  \n","$$P(w_n|w_1, ..., w_{n-1})$$\n","전체 단어 시퀀스 $W$의 확률은 모든 단어가 예측되고 나서야 알 수 있으므로 단어 시퀀스의 확률은  \n","$$P(W)=P(w_1, w_2, w_3, ..., w_n)=Π_{i=1}^n(Pw_i|w_1, ..., w_{i-1})$$"],"metadata":{"id":"yI3TcncxpnII"}},{"cell_type":"markdown","source":["# 10-2 Statistical Language Model, SLM"],"metadata":{"id":"a66vqF22p6uV"}},{"cell_type":"markdown","source":["## 1. 조건부 확률\n","\n","- 두 확률에 대해\n","$$P(B|A) = P(A,B)/P(A)$$\n","$$P(A,B) = P(A)P(B|A)$$\n","\n","- 4개의 확률일 때\n","$$P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$$\n","\n","- n개에 대해\n","$$P(x_1, x_2, x_3, ..., x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1,...,x_{n-1})$$"],"metadata":{"id":"ZGHWD-FpraV_"}},{"cell_type":"markdown","source":["## 2. 문장에 대한 확률\n","문장 'An adorable little boy is spreading smile' 의 확률 *P(An adorable little boy is spreading smiles)*를 식으로 표현\n","  \n","$P(An adorable little boy is spreading smiles)=\\\\\n","P(An)×P(adorable|An)×P(little|An\\,adorable)×\\\\...×P(smiles|An\\,adorable\\,little\\,boy\\,is\\,spreading)$"],"metadata":{"id":"2kSqXHd3r9TJ"}},{"cell_type":"markdown","source":["## 3. 카운트 기반의 접근\n","An adorable little boy가 나왔을 때, is가 나올 확률인\n","$$P(is|An\\,\\,adorable\\,\\,little\\,\\,boy)=\\frac{count(An\\,\\,adorable\\,\\,little\\,\\,boy\\,\\,is)}{count(An\\,\\,adorable\\,\\,little\\,\\,boy)}$$"],"metadata":{"id":"khL-n--QsbrP"}},{"cell_type":"markdown","source":["### 한계 - Sparsity Problem\n","카운트 기반으로 접근하려면 갖고 있는 corpus가 정말 많이 필요함  \n","위 식을 기반으로 하였을 때 An adorable little boy가 한 번도 나오지 않으면 분모가 0이 되어 확률이 정의되지 않음  \n","하지만 이렇다고 해서 현실에 사용되지 않는다는 말이 아니기 때문에 그저 충분한 데이터가 없어서 발생하는 문제  \n","-> Sparsity Problem"],"metadata":{"id":"wq-3er8uth1U"}},{"cell_type":"markdown","source":["# 10-3 N-gram Language Model\n","카운트에 기반한 통계적 접근을 사용하는 SLM의 일종  \n","But, 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용"],"metadata":{"id":"mB7s8NMTt6P6"}},{"cell_type":"markdown","source":["## 1. Corpus에서 카운트하지 못하는 경우의 감소\n","SLM의 한계: 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있음!!\n","또한, 확률을 게산하고 싶은 문장이 길어질수록 갖고있는 corpus에서 그 문장이 존재하지 않을 가능성이 높음 => 참고하는 단어를 줄여 카운트\n","<br>\n","<br>\n","$$P(is|An\\,\\,adorable\\,\\,little\\,\\,boy)≈P(is|boy)$$\n","$$P(is|An\\,\\,adorable\\,\\,little\\,\\,boy)≈P(is|little\\,\\,boy)$$"],"metadata":{"id":"JZo7Y6rmuZxW"}},{"cell_type":"markdown","source":["## 2. N-gram\n","corpus에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주\n","\n","n-gram: n개의 연속적인 단어 나열  \n",">Ex: An adorable little boy is spreading smiles\n","<br><br>\n","**uni**grams: an, adorable, little, boy, is, spreading, smiles  \n","**bi**grams: an adorable, adorable little, little boy, boy is, is spreading, spreading smiles  \n","**tri**grams: an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles  \n","**4**-grams: an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n","\n"],"metadata":{"id":"85h64XHwucq_"}},{"cell_type":"markdown","source":["n-gram을 통한 LM에서는 n-1개의 단어에만 의존  \n","예를 들어, n = 4라고 하면, 3개의 앞의 단어만을 고려해 예측  \n","~~An adorable little~~ <span style=\"color:orange\">boy is spreading</span> _\n","<br><br>\n","$$P(w|boy\\,\\,is\\,\\,spreading)=\\frac{count(boy\\,\\,is\\,\\,spreading\\,\\,w)}{count(boy\\,\\,is\\,\\,spreading)}$$"],"metadata":{"id":"_IS7Btqlv58Z"}},{"cell_type":"markdown","source":["## 3. N-gram Language Model의 한계\n","앞의 단어 몇 개만 보다 보니 의도하고 싶은대로 문장을 끝맺음하지 못하는 경우가 생김  \n","또한 문장을 읽다 보면 앞 부분과 뒷부분의 문맥이 전혀 연결이 안되는 경우도 생김  \n","\n","### (1) Sparsity Problem\n","카운트 할 수 있는 확률을 전체 문장을 보는 것보다는 높일 수 있지만, 여전히 희소문제 존재\n","\n","### (2) n을 선택하는 것은 trade-off 문제\n","앞의 몇 개의 단어를 볼 지는 결과론적 문제일 것으로 생각이 됨  \n","n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해짐  \n","또한 n이 커질수록 모델 사이즈가 커짐  \n","\n","n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되지만 근사의 정확도는 현실의 확률분포와 멀어짐\n","\n","**가능하면 n을 5이하로 잡는 것을 권장**"],"metadata":{"id":"UwwerQKowuT2"}},{"cell_type":"markdown","source":["# 10-4 한국어에서의 언어 모델"],"metadata":{"id":"4tdHK_XgYvYh"}},{"cell_type":"markdown","source":["## 1. 한국어는 어순이 중요하지 않다\n","이전 단어가 주어졌을때, 다음 단어로 어떤 단어든 등장할 수 있음  \n",">1. 나는 운동을 합니다 체육관에서  \n","2. 나는 체육관에서 운동을 합니다  \n","3. 체육관에서 운동을 합니다  \n","4. 나는 운동을 체육관에서 합니다\n"],"metadata":{"id":"_B2VtsdoY5eA"}},{"cell_type":"markdown","source":["4개의 문장은 전부 의미가 통하는 것을 볼 수 있고, 3번을 보면 '나는'을 생략해도 말이 됨  \n","즉, 한국어는 언어 모델이 제대로 다음 단어를 예측하기가 어려움"],"metadata":{"id":"_f4Ip-4GZMwW"}},{"cell_type":"markdown","source":["## 2. 한국어는 교착어이다\n","한국어에는 조사가 존재하므로 토큰화를 통해 조사나 접사를 분리하는 것이 중요한 작업이 됨"],"metadata":{"id":"4n2ZL6I3ZUa8"}},{"cell_type":"markdown","source":["## 3. 한국어는 띄어쓰기가 제대로 지켜지지 않음\n","띄어쓰기를 제대로 하지 않아도 의미가 전달되고, 띄어쓰기 규칙 또한 상대적으로 까다로운 언어이므로 자연어 처리하는 것에 있어 한국어 corpus는 띄어쓰기가 제대로 지켜지지 않는경우가 많음"],"metadata":{"id":"p87Q_9XhZhb-"}},{"cell_type":"markdown","source":["# 10-5 Perplexity\n","두 개의 모델 A, B가 있을 때 모델의 성능은 perplexity를 이용  \n","perplexity: 모델 내에서 자신의 성능을 수치화하여 결과를 내놓음"],"metadata":{"id":"qPmRQsyOZjti"}},{"cell_type":"markdown","source":["## 1. 언어모델의 평가 방법: PPL\n","PPL은 '낮을수록' 언어 모델의 성능이 좋다는 것을 의미  \n","PPL은 문장의 길이로 정규화된 문장 확률의 역수, 문장 W의 길이가 N이라고 하였을 때 PPL은 다음과 같음  \n","<br>\n","$$PPL(W)=P(w_1, w_2, w_3, ..., w_N)^{-\\frac{1}{N}}=\\frac{1}{P(w_1, w_2, w_3, ..., w_N)}^{\\frac{1}{N}}$$\n","\n","문장의 확률에 chain rule을 적용하면 아래와 같음  \n","<br>\n","$$PPL(W)=\\frac{1}{P(w_1, w_2, w_3, ..., w_N)}^{\\frac{1}{N}}=\\frac{1}{Π^N_{i=1}P(w_i|w_1, w_2, ..., w_{i-1})}^\\frac{1}{N}$$\n","\n","여기에 n-gram(bigram)을 적용하면  \n","<br>\n","$$PPL(W)=\\frac{1}{\\Pi^N_{i=1}P(w_i|w_{i-1})}^\\frac{1}{N}$$"],"metadata":{"id":"6ehq4AhfZ9-P"}},{"cell_type":"markdown","source":["## 2. 분기 계수(Branching factor)\n","PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수  \n","즉, 몇 개의 선택지를 가지고 고민하고 있는지를 의미  \n","\n","가령, 언어 모델에 어떤 테스트 데이터를 주고 측정했더니 PPL이 10이 나왔다고 할 때 해당 언어 모델은 테스트 데이터에 대해 다음 단어를 예측하는 모든 시점(time step)마다 평균 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있음  \n","\n","$$PPL(W)=P(w_1, w_2, w_3, ..., w_N)^{-\\frac{1}{N}}=(\\frac{1}{10}^N)^{-\\frac{1}{N}}=\\frac{1}{10}^{-1}=10$$"],"metadata":{"id":"nITfdONYbooj"}},{"cell_type":"code","source":[],"metadata":{"id":"bqxNdYgudkex"},"execution_count":null,"outputs":[]}]}